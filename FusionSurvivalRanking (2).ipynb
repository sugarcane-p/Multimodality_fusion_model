{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbHvq4FDiYoe"
      },
      "source": [
        "If running on Google Colab uncomment here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxpXL3w8yjCJ",
        "outputId": "71ee4407-27f6-4ff8-80c1-631bda1102f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MDvz0BOxTBZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3151c65-3a2c-4c5e-e56d-5fcfaba4ec63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.10/dist-packages (0.2.0+pt20cu118)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (2.1.1+pt20cu118)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (0.6.17+pt20cu118)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.10/dist-packages (1.6.1+pt20cu118)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt20cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.10.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.6.2)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines) (0.6.4)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines) (0.18.3)\n",
            "Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->lifelines) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric\n",
        "!pip install lifelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vcyDY-9vTjmP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler\n",
        "import torch.utils.data as data_utils\n",
        "from torchvision import datasets, transforms\n",
        "from copy import deepcopy\n",
        "from numpy.random import randn\n",
        "from torch.nn import BatchNorm1d\n",
        "from torch.nn import Sequential, Linear, ReLU,Tanh,LeakyReLU,ELU,SELU,GELU\n",
        "from torch_geometric.nn import GINConv,GATConv,EdgeConv, PNAConv,DynamicEdgeConv,global_add_pool, global_mean_pool, global_max_pool\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial import distance_matrix, Delaunay\n",
        "import random\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import pickle\n",
        "from glob import glob\n",
        "import os\n",
        "from sklearn.neighbors import kneighbors_graph, radius_neighbors_graph\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pdb\n",
        "from statistics import mean, stdev\n",
        "from glob import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch.autograd import Variable\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.neighbors import kneighbors_graph, radius_neighbors_graph\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import math\n",
        "from random import shuffle\n",
        "from itertools import islice\n",
        "from lifelines.utils import concordance_index as cindex\n",
        "from lifelines import KaplanMeierFitter\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.init import xavier_normal_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS_LOUYVTjmQ"
      },
      "source": [
        "Set up the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HhTBT3MmTjmQ"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.00002\n",
        "WEIGHT_DECAY = 0.005\n",
        "L1_WEIGHT = 0.001\n",
        "SCHEDULER = None\n",
        "BATCH_SIZE = 10\n",
        "NUM_BATCHES = 2000\n",
        "NUM_LOGS = 150 # How many times in training the loss value is stored\n",
        "\n",
        "#Select what feature set to use\n",
        "SHUFFLE_NET = True\n",
        "\n",
        "VALIDATION = True\n",
        "NORMALIZE = False\n",
        "CENSORING = True\n",
        "FRAC_TRAIN = 0.8\n",
        "CONCORD_TRACK = True\n",
        "FILTER_TRIPLE = False\n",
        "EARLY_STOPPING = True\n",
        "MODEL_PATH = 'Best_model/'\n",
        "VARIABLES = 'DSS'\n",
        "TIME_VAR = VARIABLES + '.time'\n",
        "ON_GPU = True\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "rng = np.random.default_rng()\n",
        "device = {True:'cuda:0',False:'cpu'}[USE_CUDA]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLPofWzeTjmR"
      },
      "source": [
        "Accessory methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mAl4eI-RTjmR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cuda(v):\n",
        "    if USE_CUDA:\n",
        "        return v.cuda()\n",
        "    return v\n",
        "\n",
        "def toTensor(v,dtype = torch.float,requires_grad = True):\n",
        "    return torch.from_numpy(np.array(v)).type(dtype).requires_grad_(requires_grad)\n",
        "\n",
        "def toTensorGPU(v,dtype = torch.float,requires_grad = True):\n",
        "    return cuda(torch.from_numpy(np.array(v)).type(dtype).requires_grad_(requires_grad))\n",
        "\n",
        "def toNumpy(v):\n",
        "    if type(v) is not torch.Tensor: return np.asarray(v)\n",
        "    if USE_CUDA:\n",
        "        return v.detach().cpu().numpy()\n",
        "    return v.detach().numpy()\n",
        "\n",
        "def pickleLoad(ifile):\n",
        "    with open(ifile, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def toGeometric(Gb,y,tt=1e-3):\n",
        "    return Data(x=Gb.x, edge_index=(Gb.get(W)>tt).nonzero().t().contiguous(),y=y)\n",
        "\n",
        "def toGeometricWW(X,W,y,tt=0):\n",
        "    return Data(x=toTensor(X,requires_grad = False), edge_index=(toTensor(W,requires_grad = False)>tt).nonzero().t().contiguous(),y=toTensor([y],dtype=torch.long,requires_grad = False))\n",
        "\n",
        "\n",
        "def pair_find(graphs,features):\n",
        "    indexes = []\n",
        "    for j in range(len(graphs)):\n",
        "        graph_j = graphs[j]\n",
        "        if features == 'BRCA-SHUFFLE':\n",
        "            event_j = graph_j[1][0]\n",
        "            time_j = graph_j[1][1]\n",
        "        else:\n",
        "            event_j, time_j = graph_j.event, graph_j.e_time\n",
        "        if event_j == 1:\n",
        "            for i in range(len(graphs)):\n",
        "                graph_i = graphs[i]\n",
        "                if features == 'BRCA-SHUFFLE':\n",
        "                    time_i = graph_i[1][1]\n",
        "                else:\n",
        "                    time_i = graph_i.e_time\n",
        "                if graph_j != graph_i and time_i > time_j:\n",
        "                    indexes.append((i,j))\n",
        "    shuffle(indexes)\n",
        "    return indexes\n",
        "\n",
        "def SplitBrcaData(dataset, numSplits, isShuffle, testSize):\n",
        "    if isShuffle:\n",
        "        eventVars = [dataset[i][1][0] for i in range(len(dataset))]\n",
        "    else:\n",
        "        eventVars = [int(dataset[i].event.detach().numpy()) for i in range(len(dataset))]\n",
        "    x = np.zeros(len(dataset))\n",
        "    shuffleSplit = StratifiedShuffleSplit(n_splits = numSplits, test_size = testSize, random_state=2)\n",
        "    return shuffleSplit.split(x,eventVars)\n",
        "\n",
        "def disk_graph_load(batch):\n",
        "    return [torch.load(directory + '/' + graph + '.g') for graph in batch]\n",
        "\n",
        "def get_predictions(model,graphs,model_indicator,features = 'BRCA-SHUFFLE',device=torch.device('cuda:0')) -> list:\n",
        "    outputs = []\n",
        "    e_and_t = []\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(graphs)):\n",
        "            graph = graphs[i]\n",
        "            try:\n",
        "                gene_data = graphs[i][1][2]\n",
        "            except IndexError:\n",
        "                print(f\"IndexError encountered for i={i} (Testing)\")\n",
        "            if features == 'BRCA-SHUFFLE':\n",
        "                tag = [graph[0]]\n",
        "                temp = [graph[1][0], graph[1][1]]\n",
        "                graph = disk_graph_load(tag)\n",
        "            else:\n",
        "                temp = [graph.event.item(),graph.e_time.item()]\n",
        "                graph = [graph]\n",
        "            size = 1\n",
        "            loader = DataLoader(graph, batch_size=size)\n",
        "            for d in loader:\n",
        "                d = d.to(device)\n",
        "            if model_indicator == 'combine':\n",
        "                g_loader = DataLoader([gene_data], batch_size=size)\n",
        "                for g in g_loader:\n",
        "                    g = g.to(device)\n",
        "                z,_,_ = model(g, d)\n",
        "            else:\n",
        "                z,_,_ = model(d)\n",
        "            z = toNumpy(z)\n",
        "            outputs.append(z[0][0])\n",
        "            e_and_t.append(temp)\n",
        "    return outputs, e_and_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e6OOq8D3TjmR"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, dim_features, dim_target, layers=[16,16,8],pooling='max',dropout = 0.0,conv='GINConv',gembed=False,**kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        dim_features : TYPE Int\n",
        "            DESCRIPTION. Number of features of each node\n",
        "        dim_target : TYPE Int\n",
        "            DESCRIPTION. Number of outputs\n",
        "        layers : TYPE, optional List of number of nodes in each layer\n",
        "            DESCRIPTION. The default is [6,6].\n",
        "        pooling : TYPE, optional\n",
        "            DESCRIPTION. The default is 'max'.\n",
        "        dropout : TYPE, optional\n",
        "            DESCRIPTION. The default is 0.0.\n",
        "        conv : TYPE, optional Layer type string {'GINConv','EdgeConv'} supported\n",
        "            DESCRIPTION. The default is 'GINConv'.\n",
        "        gembed : TYPE, optional Graph Embedding\n",
        "            DESCRIPTION. The default is False. Pool node scores or pool node features\n",
        "        **kwargs : TYPE\n",
        "            DESCRIPTION.\n",
        "        Raises\n",
        "        ------\n",
        "        NotImplementedError\n",
        "            DESCRIPTION.\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "        \"\"\"\n",
        "        super(GNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.embeddings_dim=layers\n",
        "        self.no_layers = len(self.embeddings_dim)\n",
        "        self.first_h = []\n",
        "        self.nns = []\n",
        "        self.convs = []\n",
        "        self.linears = []\n",
        "        self.pooling = {'max':global_max_pool,'mean':global_mean_pool,'add':global_add_pool}[pooling]\n",
        "        self.gembed = gembed #if True then learn graph embedding for final classification (classify pooled node features) otherwise pool node decision scores\n",
        "\n",
        "        for layer, out_emb_dim in enumerate(self.embeddings_dim):\n",
        "            if layer == 0:\n",
        "                self.first_h = Sequential(Linear(dim_features, out_emb_dim), BatchNorm1d(out_emb_dim),GELU())\n",
        "                self.linears.append(Sequential(Linear(out_emb_dim, dim_target),GELU()))\n",
        "\n",
        "            else:\n",
        "                input_emb_dim = self.embeddings_dim[layer-1]\n",
        "                self.linears.append(Linear(out_emb_dim, dim_target))\n",
        "                subnet = Sequential(Linear(input_emb_dim, out_emb_dim), BatchNorm1d(out_emb_dim))\n",
        "                if conv=='GINConv':\n",
        "                    self.nns.append(subnet)\n",
        "                    self.convs.append(GINConv(self.nns[-1], **kwargs))  # Eq. 4.2 eps=100, train_eps=False\n",
        "                elif conv=='EdgeConv':\n",
        "                    subnet = Sequential(Linear(2*input_emb_dim, out_emb_dim), BatchNorm1d(out_emb_dim))\n",
        "                    self.nns.append(subnet)\n",
        "                    self.convs.append(EdgeConv(self.nns[-1],**kwargs))#DynamicEdgeConv#EdgeConv                aggr='mean'\n",
        "                elif conv=='GATConv':\n",
        "                    self.nns.append(subnet)\n",
        "                    self.convs.append(GATConv(input_emb_dim, out_emb_dim, heads=2, concat=False, dropout=dropout))\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "        self.nns = torch.nn.ModuleList(self.nns)\n",
        "        self.convs = torch.nn.ModuleList(self.convs)\n",
        "        self.linears = torch.nn.ModuleList(self.linears)  # has got one more for initial input\n",
        "\n",
        "    def forward(self, data) -> torch.tensor:\n",
        "\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        out = 0\n",
        "        pooling = self.pooling\n",
        "        Z = 0\n",
        "        for layer in range(self.no_layers):\n",
        "            if layer == 0:\n",
        "                x = self.first_h(x)\n",
        "                z = self.linears[layer](x)\n",
        "                Z+=z\n",
        "                dout = F.dropout(pooling(z, batch), p=self.dropout, training=self.training)\n",
        "                out += dout\n",
        "            else:\n",
        "                x = self.convs[layer-1](x,edge_index)\n",
        "                if not self.gembed:\n",
        "                    z = self.linears[layer](x)\n",
        "                    Z+=z\n",
        "                    dout = F.dropout(pooling(z, batch), p=self.dropout, training=self.training)\n",
        "                else:\n",
        "                    dout = F.dropout(self.linears[layer](pooling(x, batch)), p=self.dropout, training=self.training)\n",
        "                out += dout\n",
        "\n",
        "        return out,Z,x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0lVN0RY4gzGy"
      },
      "outputs": [],
      "source": [
        "class GeneSubNet(torch.nn.Module):\n",
        "    def __init__(self, dim_features, hidden_dims, dropout = 0.0,**kwargs) -> None:\n",
        "        super(GeneSubNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        input_dim = dim_features\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(Linear(input_dim, hidden_dim))\n",
        "            layers.append(BatchNorm1d(hidden_dim))\n",
        "            layers.append(ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            input_dim = hidden_dim\n",
        "        self.linear_relu_stack = Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_relu_stack(x)\n",
        "\n",
        "class LMF(nn.Module):\n",
        "    def __init__(self, gene_in, graph_in, hidden_dims, dropouts, output_dim, rank):\n",
        "\n",
        "        super(LMF, self).__init__()\n",
        "\n",
        "        self.gene_in = gene_in\n",
        "        self.graph_in = graph_in\n",
        "\n",
        "        self.gene_hidden = hidden_dims[0]\n",
        "        self.graph_hidden = hidden_dims[1]\n",
        "        self.output_dim = output_dim\n",
        "        self.rank = rank\n",
        "\n",
        "        self.gene_prob = dropouts[0]\n",
        "        self.graph_prob = dropouts[1]\n",
        "        self.post_fusion_prob = dropouts[2]\n",
        "\n",
        "        # define the pre-fusion subnetworks\n",
        "        self.gene_subnet = GeneWithSAE(self.gene_in, round(self.gene_in*0.5), self.gene_hidden)\n",
        "        self.graph_subnet = GNN(dim_features=self.graph_in,\n",
        "                               dim_target = self.graph_hidden[-1], layers = self.graph_hidden[:-1],\n",
        "                               dropout = 0.0, pooling = 'mean', conv='GATConv', aggr = 'max')\n",
        "\n",
        "        # define the post_fusion layers\n",
        "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
        "        # define the fusion factors as trainable parameters, the \"+1\" is to include a bias term.\n",
        "        self.gene_factor = Parameter(torch.Tensor(self.rank, self.gene_hidden[-1] + 1, self.output_dim))\n",
        "        self.graph_factor = Parameter(torch.Tensor(self.rank, self.graph_hidden[-1] + 1, self.output_dim))\n",
        "        self.fusion_weights = Parameter(torch.Tensor(1, self.rank))\n",
        "        self.fusion_bias = Parameter(torch.Tensor(1, self.output_dim))\n",
        "\n",
        "        # init teh factors\n",
        "        xavier_normal_(self.gene_factor)\n",
        "        xavier_normal_(self.graph_factor)\n",
        "        xavier_normal_(self.fusion_weights)\n",
        "        self.fusion_bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, gene_x, graph_x):\n",
        "\n",
        "        gene_h,decoded_gene = self.gene_subnet(gene_x)\n",
        "        graph_h, _,_ = self.graph_subnet(graph_x)\n",
        "        batch_size = gene_h.data.shape[0]\n",
        "\n",
        "        # next we perform low-rank multimodal fusion\n",
        "        # here is a more efficient implementation than the one the paper describes\n",
        "        # basically swapping the order of summation and elementwise product\n",
        "        if gene_h.is_cuda:\n",
        "            DTYPE = torch.cuda.FloatTensor\n",
        "        else:\n",
        "            DTYPE = torch.FloatTensor\n",
        "\n",
        "        #Append a bias (all ones) to the gene and graph hidden layers.\n",
        "        _gene_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), gene_h), dim=1)\n",
        "        _graph_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), graph_h), dim=1)\n",
        "\n",
        "        #The product of the data with the respective fusion factors.\n",
        "        fusion_gene = torch.matmul(_gene_h, self.gene_factor)\n",
        "        fusion_graph = torch.matmul(_graph_h, self.graph_factor)\n",
        "        fusion_zy = fusion_gene * fusion_graph\n",
        "\n",
        "        # output = torch.sum(fusion_zy, dim=0).squeeze()\n",
        "        # use linear transformation instead of simple summation, more flexibility\n",
        "        output = torch.matmul(self.fusion_weights, fusion_zy.permute(1, 0, 2)).squeeze() + self.fusion_bias\n",
        "        output = output.view(-1, self.output_dim)\n",
        "        return output, decoded_gene,fusion_zy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BuFPWEwGOCK8"
      },
      "outputs": [],
      "source": [
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, round(encoding_dim*0.75)),\n",
        "            nn.BatchNorm1d(round(encoding_dim*0.75)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(round(encoding_dim*0.75), encoding_dim),\n",
        "            nn.BatchNorm1d(encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, round(input_dim*0.75)),\n",
        "            nn.BatchNorm1d(round(input_dim*0.75)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(round(input_dim*0.75), input_dim),\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "class GeneWithSAE(nn.Module):\n",
        "    def __init__(self, sae_input_dim, sae_encoding_dim, hidden_dims, **kwargs):\n",
        "        super(GeneWithSAE, self).__init__()\n",
        "        self.sae = SparseAutoencoder(sae_input_dim, sae_encoding_dim)\n",
        "        # Update the GNN to take encoded feature dimensions\n",
        "        self.gene = GeneSubNet(sae_encoding_dim, hidden_dims, **kwargs)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Using the SAE to get encoded features\n",
        "        encoded, decoded = self.sae(data)\n",
        "        gene_output = self.gene(encoded)\n",
        "        return gene_output, decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "t_cRxCyzTjmS"
      },
      "outputs": [],
      "source": [
        "class NetWrapper:\n",
        "    def __init__(self, model, device='cuda:0',features='BRCA-CC',model_indicator = 'graph', encoder = 'True') -> None:\n",
        "        self.model = model\n",
        "        self.device = torch.device(device)\n",
        "        self.features = features\n",
        "        self.model_indicator = model_indicator\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def loss_fn(self,batch,gene_tensors,optimizer) -> float:\n",
        "        z = toTensorGPU(0)\n",
        "        loss = 0\n",
        "        unzipped = [j for pair in batch for j in pair]\n",
        "        # This can be changed when using a system with large RAM\n",
        "        if self.features == 'BRCA-SHUFFLE':\n",
        "            graph_set = list(set(unzipped))\n",
        "            graphs = disk_graph_load(graph_set)\n",
        "            unzipped = None\n",
        "        else:\n",
        "            graph_set = unzipped\n",
        "            graphs = graph_set\n",
        "            unzipped = None\n",
        "        batch_load = DataLoader(graphs, batch_size = len(graphs))\n",
        "        for d in batch_load:\n",
        "            d = d.to(self.device)\n",
        "\n",
        "        if self.model_indicator == 'combine':\n",
        "            gene_tensors = DataLoader(gene_tensors, batch_size = len(gene_tensors))\n",
        "            for e in gene_tensors:\n",
        "                e = e.to(self.device)\n",
        "\n",
        "        self.model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if self.model_indicator == 'combine':\n",
        "            output,decoded_gene,_ = self.model(e, d)\n",
        "        else:\n",
        "            output,_,_ = self.model(d)\n",
        "        num_pairs = len(batch)\n",
        "        for (xi,xj) in batch:\n",
        "            graph_i, graph_j = graph_set.index(xi), graph_set.index(xj)\n",
        "            # Compute loss function\n",
        "            dz = output[graph_i] - output[graph_j]\n",
        "            loss += torch.max(z, 1.0 - dz)\n",
        "\n",
        "        survival_loss = loss/num_pairs\n",
        "        if self.encoder == 'True':\n",
        "            if self.model_indicator == 'graph':\n",
        "                loss = survival_loss\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                return loss.item()\n",
        "            else:\n",
        "                # Reconstruction losses\n",
        "                recon_loss_gene = F.mse_loss(decoded_gene, e)\n",
        "                # Combine everything\n",
        "                loss = survival_loss + 0.8 * recon_loss_gene\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                return loss.item()\n",
        "\n",
        "    def validation_loss_and_Cindex_eval(self,graphs,pairs) -> float:\n",
        "        tot_loss = 0\n",
        "        print('Number of Validation Pairs: ' + str(len(pairs)))\n",
        "        predictions, e_and_t = get_predictions(self.model,graphs,self.features)\n",
        "        for j in range(len(pairs)):\n",
        "            p_graph_i = predictions[pairs[j][0]]\n",
        "            p_graph_j = predictions[pairs[j][1]]\n",
        "            dz = p_graph_i - p_graph_j\n",
        "            loss = max(0, 1.0 - dz)\n",
        "            tot_loss += loss\n",
        "        epoch_val_loss = tot_loss / len(pairs)\n",
        "        T = [x[1] for x in e_and_t]\n",
        "        E = [x[0] for x in e_and_t]\n",
        "        concord = cindex(T,predictions,E)\n",
        "        return epoch_val_loss, concord\n",
        "\n",
        "    def censor_data(self,graphs, censor_time): # The censor time measured in years\n",
        "        cen_time = 365 * censor_time\n",
        "        for graph in graphs:\n",
        "            if self.features == 'BRCA-SHUFFLE':\n",
        "                time = graph[1][1]\n",
        "            else:\n",
        "                time = graph.e_time\n",
        "            if time > cen_time:\n",
        "                if self.features == 'BRCA-SHUFFLE':\n",
        "                    graph[1] = (0,cen_time,graph[1][2])\n",
        "                else:\n",
        "                    graph.event = toTensor(0)\n",
        "                    graph.e_time = toTensor(cen_time)\n",
        "            else:\n",
        "                continue\n",
        "        return graphs\n",
        "\n",
        "    def train(self,training_data,validation_data,max_batches=500,num_logs=50,optimizer=torch.optim.Adam,\n",
        "              early_stopping = 10, return_best = False, batch_size = 10) -> float:\n",
        "            return_best = return_best and validation_data is not None\n",
        "            counter = 0 # To resolve list index errors with large NUM_BATCHES vals\n",
        "            log_interval = max_batches // num_logs\n",
        "            loss_vals = {}\n",
        "            loss_vals['train'] = []\n",
        "            loss_vals['validation'] = []\n",
        "            concords = []\n",
        "            c_best = 0.5\n",
        "            best_batch = 1000\n",
        "            patience = early_stopping\n",
        "            training_indexes = pair_find(training_data,self.features)\n",
        "            print(\"Number of batches used for training \"+ str(max_batches))\n",
        "            print('Num Pairs: ' + str(len(training_indexes)))\n",
        "            best_model = deepcopy(self.model)\n",
        "            for i in tqdm(range(1,max_batches + 1)):\n",
        "                if counter < len(training_indexes) - batch_size:\n",
        "                    batch_pairs = []\n",
        "                    gene_tensors = []\n",
        "                    index_pairs = training_indexes[counter:counter+batch_size]\n",
        "                    for j in range(len(index_pairs)):\n",
        "                        if self.features == 'BRCA-SHUFFLE':\n",
        "                            graph_i = training_data[index_pairs[j][0]][0]\n",
        "                            graph_j = training_data[index_pairs[j][1]][0]\n",
        "                            gene_i = training_data[index_pairs[j][0]][1][2]\n",
        "                            gene_j = training_data[index_pairs[j][1]][1][2]\n",
        "                        batch_pairs.append((graph_i,graph_j))\n",
        "                        gene_tensors.append(gene_i)\n",
        "                        gene_tensors.append(gene_j)\n",
        "                    gene_tensors = list(set(gene_tensors))\n",
        "                    loss = self.loss_fn(batch_pairs,gene_tensors,optimizer)\n",
        "                    counter += batch_size\n",
        "                else:\n",
        "                    counter = 0\n",
        "                loss_vals['train'].append(loss)\n",
        "                if i % log_interval == 0:\n",
        "                    if validation_data is not None:\n",
        "                        val_loss, c_val = self.validation_loss_and_Cindex_eval(validation_data,validation_indexes)\n",
        "                        loss_vals['validation'].append(val_loss)\n",
        "                        concords.append(c_val)\n",
        "                        print(\"Current Vali Loss Val: \" + str(val_loss) + \"\\n\")\n",
        "                        print(\"\\n\" + \"Current Loss Val: \" + str(loss) + \"\\n\")\n",
        "                        if return_best and c_val > c_best:\n",
        "                            c_best = c_val\n",
        "                            #best_model = deepcopy(model)\n",
        "                            best_batch = i\n",
        "                        if i - best_batch > patience*log_interval:\n",
        "                            print(\"Early Stopping\")\n",
        "                            #break\n",
        "            return loss_vals, concords, self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x3GyQZdoTjmT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, model, model_indicator, device='cuda:0',features = 'BRCA-SHUFFLE') -> None:\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.features = features\n",
        "        self.model_indicator = model_indicator\n",
        "\n",
        "    def get_predictions(self,model,graphs,device=torch.device('cuda:0')) -> list:\n",
        "        outputs = []\n",
        "        e_and_t = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(graphs)):\n",
        "                graph = graphs[i]\n",
        "                if self.features == 'BRCA-SHUFFLE':\n",
        "                    tag = [graph[0]]\n",
        "                    temp = [graph[1], graph[1]]\n",
        "                    graph = disk_graph_load(tag)\n",
        "                else:\n",
        "                    temp = [graph.event.item(),graph.e_time.item()]\n",
        "                    graph = [graph]\n",
        "                size = 1\n",
        "                loader = DataLoader(graph, batch_size=size)\n",
        "                for d in loader:\n",
        "                    d = d.to(device)\n",
        "                z = model(d)\n",
        "                z = toNumpy(z)\n",
        "                outputs.append(z[0])\n",
        "                e_and_t.append(temp)\n",
        "        return outputs, e_and_t\n",
        "\n",
        "    def test_evaluation(self,testDataset):\n",
        "        predictions, e_and_t = get_predictions(self.model,testDataset,self.model_indicator,self.features)\n",
        "        T = [x[1] for x in e_and_t]\n",
        "        E = [x[0] for x in e_and_t]\n",
        "        concord = cindex(T,predictions,E)\n",
        "        return concord\n",
        "\n",
        "    def K_M_Curves(self, graphs, split_val, mode = 'Train') -> None:\n",
        "        outputs, e_and_t = get_predictions(self.model,graphs,self.features)\n",
        "        T = [x[1] for x in e_and_t]\n",
        "        E = [x[0] for x in e_and_t]\n",
        "        mid = np.median(outputs)\n",
        "        if mode != 'Train':\n",
        "            if split_val > 0:\n",
        "                mid = split_val\n",
        "        else:\n",
        "            print(mid)\n",
        "        T_high = []\n",
        "        T_low = []\n",
        "        E_high = []\n",
        "        E_low = []\n",
        "        for i in range(len(outputs)):\n",
        "          if outputs[i] <= mid:\n",
        "            T_high.append(T[i])\n",
        "            E_high.append(E[i])\n",
        "          else:\n",
        "            T_low.append(T[i])\n",
        "            E_low.append(E[i])\n",
        "        km_high = KaplanMeierFitter()\n",
        "        km_low = KaplanMeierFitter()\n",
        "        ax = plt.subplot(111)\n",
        "        ax = km_high.fit(T_high, event_observed=E_high, label = 'High').plot_survival_function(ax=ax)\n",
        "        ax = km_low.fit(T_low, event_observed=E_low, label = 'Low').plot_survival_function(ax=ax)\n",
        "        from lifelines.plotting import add_at_risk_counts\n",
        "        add_at_risk_counts(km_high, km_low, ax=ax)\n",
        "        plt.title('Kaplan-Meier estimate')\n",
        "        plt.ylabel('Survival probability')\n",
        "        plt.show()\n",
        "        plt.tight_layout()\n",
        "        from lifelines.statistics import logrank_test\n",
        "        results = logrank_test(T_low, T_high, E_low, E_high)\n",
        "        print(\"p-value %s; log-rank %s\" % (results.p_value, np.round(results.test_statistic, 6)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZBN6My4WTjmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9602d052-4c9d-4b16-c935-50e2d30b5b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 652/652 [01:46<00:00,  6.12it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    device = {True:'cuda:0',False:'cpu'}[USE_CUDA]\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    from natsort import natsorted\n",
        "    # This is set up to run on colab vvv\n",
        "    survival_file = r'drive/MyDrive/SlideGraph/NIHMS978596-supplement-1.xlsx'\n",
        "    #gene_file = pd.read_csv('drive/MyDrive/SlideGraph/topics_brca_dawood.csv')\n",
        "    gene_file = pd.read_excel('drive/MyDrive/SlideGraph/656Genes_data.xlsx')\n",
        "    gene_file.set_index(gene_file.columns[0], inplace=True)\n",
        "    columns_to_drop = ['ER', 'PR', 'HER2', 'OS', 'OSTime', 'PFI', 'PFITime', 'DSS', 'DSSTime']\n",
        "    gene_file = gene_file.drop(columns=columns_to_drop)\n",
        "    # Log-transform the whole dataset\n",
        "    gene_file = gene_file.applymap(lambda x: np.log(1 + x))\n",
        "    # Z-score normalization for each column (each gene across all patients)\n",
        "    gene_file = (gene_file - gene_file.mean()) / gene_file.std()\n",
        "    # Define a function to categorize each column\n",
        "    def categorize_column(col):\n",
        "        upper_threshold = col.mean() + 1 * col.std()\n",
        "        lower_threshold = col.mean() - 1 * col.std()\n",
        "        return col.apply(lambda x: 1 if x > upper_threshold else (-1 if x < lower_threshold else 0))\n",
        "    # Apply the function to each column in gene_file\n",
        "    gene_file = gene_file.apply(categorize_column)\n",
        "    cols2read = [VARIABLES,TIME_VAR]\n",
        "    TS = pd.read_excel(survival_file).rename(columns= {'bcr_patient_barcode':'ID'}).set_index('ID')  # path to clinical file\n",
        "    TS = TS[cols2read][TS.type == 'BRCA']\n",
        "    # Conditionally set TIME_VAR\n",
        "    TS.loc[TS[TIME_VAR] >= 3650, TIME_VAR] = 3650\n",
        "    if SHUFFLE_NET:\n",
        "        bdir = r'drive/MyDrive/SlideGraph/ShuffleNet_0.8_dth_4K/'\n",
        "        # Set up directory for on disk dataset\n",
        "        directory = r'drive/MyDrive/SlideGraph/Graphs'\n",
        "        try:\n",
        "            os.mkdir(directory)\n",
        "        except FileExistsError:\n",
        "            pass\n",
        "    Exid = 'Slide_Graph CC_feats'\n",
        "    graphlist = glob(os.path.join(bdir, \"*.pkl\"))#[0:100]\n",
        "    device = 'cuda:0'\n",
        "    cpu = torch.device('cpu')\n",
        "\n",
        "    if FILTER_TRIPLE:\n",
        "        filter_file = 'drive/MyDrive/SlideGraph/TCGA-BRCA-DX_CLINI (8).xlsx'\n",
        "        cols = ['ERStatus','PRStatus','HER2FinalStatus']\n",
        "        db = pd.read_excel(filter_file).rename(columns= {'CompleteTCGAID':'ID'}).set_index('ID')\n",
        "        db = db[cols]\n",
        "\n",
        "    try:\n",
        "        os.mkdir(MODEL_PATH)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "    graphlist = natsorted(graphlist)\n",
        "    dataset = []\n",
        "    from tqdm import tqdm\n",
        "    for graph in tqdm(graphlist):\n",
        "        TAG = os.path.split(graph)[-1].split('_')[0][:12]\n",
        "        status = TS.loc[TAG,:][1]\n",
        "        event, event_time = TS.loc[TAG,:][0], TS.loc[TAG,:][1]\n",
        "\n",
        "        gene = gene_file.loc[TAG]\n",
        "        gene = torch.tensor(gene.to_numpy(), dtype=torch.float)\n",
        "\n",
        "        if np.isnan(event):\n",
        "            continue\n",
        "        if SHUFFLE_NET:\n",
        "            G = pickleLoad(graph)\n",
        "            # Google Colab may sometimes produce \"Transport endpoint is not connected\" error here\n",
        "            # This is not a bug in the code. Rerunning the cell will fix this.\n",
        "            G.to('cpu')\n",
        "            gene.to('cpu')\n",
        "        else:\n",
        "            if USE_CUDA:\n",
        "                G = pickleLoad(graph)\n",
        "                G.to('cpu')\n",
        "                gene.to('cpu')\n",
        "            else:\n",
        "                G = torch.load(graph, map_location=device)\n",
        "        try:\n",
        "            G.y = toTensorGPU([int(status)], dtype=torch.long, requires_grad = False)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        W = radius_neighbors_graph(toNumpy(G.coords), 1500, mode=\"connectivity\",include_self=False).toarray()\n",
        "        g = toGeometricWW(toNumpy(G.x),W,toNumpy(G.y))\n",
        "        g.coords = G.coords\n",
        "        g.event = toTensor(event)\n",
        "        g.e_time = toTensor(event_time)\n",
        "        if SHUFFLE_NET:\n",
        "            dataset.append([TAG,(event,event_time,gene)])\n",
        "            torch.save(g,directory+'/'+TAG+'.g')\n",
        "        else:\n",
        "            dataset.append(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "txry3EFjTjmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bfe779c-fe6e-4933-c845-4dff3de240ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches used for training 2000\n",
            "Num Pairs: 10221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [11:01<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6802413273001509\n",
            "Number of batches used for training 2000\n",
            "Num Pairs: 10291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [11:34<00:00,  2.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6894502228826151\n",
            "Number of batches used for training 2000\n",
            "Num Pairs: 9852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [11:01<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6470588235294118\n",
            "Number of batches used for training 2000\n",
            "Num Pairs: 9764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [11:18<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8022388059701493\n",
            "Number of batches used for training 2000\n",
            "Num Pairs: 10552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [10:40<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6974921630094044\n",
            "Performance on test data over 5 folds:\n",
            "0.7032962685383463 +/- 0.058543161153470774\n",
            "perf on each split was: [0.6802413273001509, 0.6894502228826151, 0.6470588235294118, 0.8022388059701493, 0.6974921630094044]\n"
          ]
        }
      ],
      "source": [
        "trainingDataset = dataset\n",
        "event_vector = np.array([int(g[1][0]) for g in trainingDataset])\n",
        "\n",
        "folds = 5\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "if SHUFFLE_NET:\n",
        "    G = torch.load(directory + f'/{dataset[0][0]}.g')\n",
        "else:\n",
        "    G = dataset[0]\n",
        "\n",
        "converg_vals = []\n",
        "fold_concord = []\n",
        "eval_metrics = []\n",
        "\n",
        "for train_index, vali_index in SplitBrcaData(trainingDataset,folds,SHUFFLE_NET,0.2):\n",
        "    # get indices for training and testing\n",
        "\n",
        "    # Set up model and optimizer\n",
        "        model = LMF(gene_in = len(dataset[0][1][2]), graph_in = G.x.shape[1],\n",
        "                         hidden_dims = [[256,128,64,32,16,1], [128,64,32,16,1]], dropouts = [0.0, 0.0, 0.0],\n",
        "                        output_dim = 1, rank = 4)\n",
        "        net = NetWrapper(model,device = device,features = 'BRCA-SHUFFLE', model_indicator='combine', encoder='True')\n",
        "        '''Remove the # in the model, net and eval for testing GNN network'''\n",
        "        #model = GNN(dim_features=G.x.shape[1],\n",
        "        #            dim_target = 1, layers = [256,128,64,32,16,1],\n",
        "        #            dropout = 0.0, pooling = 'mean', conv='GATConv', aggr = 'max')\n",
        "        #net = NetWrapper(model,device = device,features = 'BRCA-SHUFFLE', model_indicator='graph', encoder='False')\n",
        "        model = model.to(device)\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=LEARNING_RATE,\n",
        "                        weight_decay=WEIGHT_DECAY)\n",
        "        x_train = [trainingDataset[i] for i in train_index]\n",
        "        testDataset = [trainingDataset[i] for i in vali_index]\n",
        "        # Only censoring the test data\n",
        "        # x_val = net.censor_data(x_val,10)\n",
        "        losses, concords, BestModel = net.train(x_train,None,optimizer = optimizer,return_best = True,max_batches = NUM_BATCHES)\n",
        "        # Evaluate\n",
        "        testDataset = net.censor_data(testDataset,10)\n",
        "        eval = Evaluator(BestModel,model_indicator='combine',device='cuda:0',features='BRCA-SHUFFLE')\n",
        "        #eval = Evaluator(BestModel,model_indicator='graph',device='cuda:0',features='BRCA-SHUFFLE')\n",
        "        concord = eval.test_evaluation(testDataset)\n",
        "        print(concord)\n",
        "        eval_metrics.append(concord)\n",
        "        converg_vals.append(losses)\n",
        "        fold_concord.append(concords)\n",
        "        #m = max(concords)\n",
        "\n",
        "avg_c = mean(eval_metrics)\n",
        "stdev_c = stdev(eval_metrics)\n",
        "\n",
        "print(f\"Performance on test data over {folds} folds:\")\n",
        "print(f\"{avg_c} +/- {stdev_c}\")\n",
        "print(f\"perf on each split was: {eval_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4gPJ6XIS56c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}